Metadata-Version: 2.4
Name: kestrel
Version: 0.1.0
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: ipykernel>=6.29.5
Requires-Dist: torch>=2.6.0
Requires-Dist: transformers>=4.48.3
Requires-Dist: transformers-cfg@ git+https://github.com/epfl-dlab/transformers-CFG.git@main
Requires-Dist: accelerate>=1.4.0
Requires-Dist: ipywidgets>=8.1.5
Requires-Dist: xgrammar==0.1.11
Requires-Dist: ninja>=1.11.1.3
Requires-Dist: matplotlib>=3.10.0
Requires-Dist: flameprof>=0.4
Requires-Dist: pytest>=8.3.5
Requires-Dist: jsonschema>=4.23.0
Requires-Dist: maturin[zig]>=1.8.2
Requires-Dist: bitsandbytes>=0.45.3
Requires-Dist: psutil>=7.0.0
Requires-Dist: llguidance>=0.6.31
Requires-Dist: openai>=1.74.0
Requires-Dist: tqdm>=4.67.1
Requires-Dist: grammarinator>=23.7
Dynamic: license-file

# kestrel
A simple fuzzer using LLM + GCD/

1. Analyze the existing test corpus to extract syntactic and semantic information.

```bash
python scripts/analyze_corpus.py \
    examples/mlir/general \
    mlir-opt \
    --grammar-file grammars/mlir.lark \
    --model-name meta-llama/Llama-3.1-8B-Instruct \
    --num-analysis-samples 10 \
    --output-dir output
```

2. Generate seeds using cached analysis.
   
```bash
python scripts/generate_seeds.py \
    examples/mlir/general \
    mlir-opt \
    --grammar-file grammars/mlir.lark \
    --model-name meta-llama/Llama-3.1-8B-Instruct \
    --num-samples 100 \
    --num-few-shot 3 \
    --prompt-refresh-interval 10 \
    --filter-with-judge \
    --quality-threshold 0.5 \
    --output-dir output/seeds
```
