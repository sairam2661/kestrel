[Generation]
# Model configuration
model_name = meta-llama/Llama-3.1-8B-Instruct
judge_model = meta-llama/Llama-3.1-8B-Instruct

# Target tool
tool_name = mlir-opt
focus_dialects = 

# File paths
grammar_file = grammars/mlir.lark
corpus_dir = examples/mlir/general
output_dir = samples/mlir/general
cache_dir = output/cache

# Generation parameters
max_new_tokens = 1024
num_samples_to_generate = 1000
num_few_shot_examples = 5
prompt_refresh_interval = 10
temperature = 0.8

# Judge filtering (optional)
use_judge = false
quality_threshold = 0.7

# Analysis parameters (for analyze_corpus.py)
num_analysis_samples = 10
extract_operations = false